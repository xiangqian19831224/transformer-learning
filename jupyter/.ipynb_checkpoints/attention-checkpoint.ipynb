{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.rand(128,32,512)\n",
    "d_model=512\n",
    "n_head=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_combine = nn.Linear(d_model, d_model)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch, time, dimension = q.shape\n",
    "        n_d = self.d_model // self.n_head\n",
    "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
    "        q = q.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)\n",
    "        k = k.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)\n",
    "        v = v.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)\n",
    "        score = q @ k.transpose(2, 3) / math.sqrt(n_d)\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask == 0, -10000)\n",
    "        score = self.softmax(score) @ v\n",
    "        score = score.permute(0, 2, 1, 3).contiguous().view(batch, time, dimension)\n",
    "        out = self.w_combine(score)\n",
    "        return out\n",
    "\n",
    "# 定义模型的维度和头数\n",
    "d_model = 512\n",
    "n_head = 8\n",
    "\n",
    "# 创建多头注意力实例\n",
    "attention = MultiHeadAttention(d_model, n_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0490,  0.0948,  0.1095,  ...,  0.1291,  0.2857,  0.3525],\n",
      "         [-0.0493,  0.0957,  0.1098,  ...,  0.1290,  0.2850,  0.3530],\n",
      "         [-0.0493,  0.0940,  0.1099,  ...,  0.1286,  0.2857,  0.3524],\n",
      "         ...,\n",
      "         [-0.0496,  0.0955,  0.1101,  ...,  0.1289,  0.2863,  0.3530],\n",
      "         [-0.0491,  0.0937,  0.1101,  ...,  0.1287,  0.2851,  0.3524],\n",
      "         [-0.0489,  0.0942,  0.1093,  ...,  0.1300,  0.2864,  0.3527]],\n",
      "\n",
      "        [[-0.0634,  0.1326,  0.1045,  ...,  0.1487,  0.2822,  0.3802],\n",
      "         [-0.0635,  0.1312,  0.1038,  ...,  0.1496,  0.2823,  0.3798],\n",
      "         [-0.0632,  0.1319,  0.1037,  ...,  0.1479,  0.2820,  0.3808],\n",
      "         ...,\n",
      "         [-0.0645,  0.1320,  0.1048,  ...,  0.1484,  0.2825,  0.3810],\n",
      "         [-0.0640,  0.1320,  0.1045,  ...,  0.1483,  0.2827,  0.3810],\n",
      "         [-0.0628,  0.1318,  0.1039,  ...,  0.1497,  0.2821,  0.3804]],\n",
      "\n",
      "        [[-0.0686,  0.1075,  0.0883,  ...,  0.1295,  0.2744,  0.3690],\n",
      "         [-0.0690,  0.1077,  0.0880,  ...,  0.1308,  0.2748,  0.3690],\n",
      "         [-0.0696,  0.1071,  0.0889,  ...,  0.1294,  0.2738,  0.3684],\n",
      "         ...,\n",
      "         [-0.0685,  0.1075,  0.0876,  ...,  0.1304,  0.2745,  0.3701],\n",
      "         [-0.0690,  0.1070,  0.0886,  ...,  0.1300,  0.2743,  0.3686],\n",
      "         [-0.0697,  0.1068,  0.0891,  ...,  0.1303,  0.2754,  0.3692]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0515,  0.0849,  0.0892,  ...,  0.1257,  0.2980,  0.3684],\n",
      "         [-0.0524,  0.0844,  0.0890,  ...,  0.1262,  0.2984,  0.3677],\n",
      "         [-0.0516,  0.0847,  0.0881,  ...,  0.1261,  0.2988,  0.3684],\n",
      "         ...,\n",
      "         [-0.0530,  0.0851,  0.0880,  ...,  0.1265,  0.2986,  0.3685],\n",
      "         [-0.0517,  0.0848,  0.0881,  ...,  0.1256,  0.2977,  0.3678],\n",
      "         [-0.0522,  0.0854,  0.0891,  ...,  0.1256,  0.2979,  0.3680]],\n",
      "\n",
      "        [[-0.0606,  0.0885,  0.1235,  ...,  0.1248,  0.2792,  0.3612],\n",
      "         [-0.0603,  0.0887,  0.1228,  ...,  0.1245,  0.2795,  0.3609],\n",
      "         [-0.0605,  0.0889,  0.1237,  ...,  0.1245,  0.2791,  0.3608],\n",
      "         ...,\n",
      "         [-0.0604,  0.0887,  0.1235,  ...,  0.1248,  0.2798,  0.3618],\n",
      "         [-0.0607,  0.0889,  0.1232,  ...,  0.1243,  0.2794,  0.3611],\n",
      "         [-0.0608,  0.0888,  0.1235,  ...,  0.1239,  0.2795,  0.3607]],\n",
      "\n",
      "        [[-0.0666,  0.1059,  0.0846,  ...,  0.1545,  0.2946,  0.3733],\n",
      "         [-0.0661,  0.1057,  0.0851,  ...,  0.1537,  0.2946,  0.3728],\n",
      "         [-0.0658,  0.1059,  0.0845,  ...,  0.1538,  0.2939,  0.3734],\n",
      "         ...,\n",
      "         [-0.0661,  0.1054,  0.0837,  ...,  0.1544,  0.2945,  0.3730],\n",
      "         [-0.0655,  0.1055,  0.0840,  ...,  0.1543,  0.2941,  0.3744],\n",
      "         [-0.0645,  0.1055,  0.0844,  ...,  0.1550,  0.2944,  0.3741]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out=attention(x,x,x)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
